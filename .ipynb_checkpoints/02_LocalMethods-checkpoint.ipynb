{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - K-Nearest Neighbours\n",
    "\n",
    "This lab is about the implementation and analysis of the KNN algorithm for classification and regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'griddata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86c0a18062c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgriddata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'griddata'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.mlab import griddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation for binary classification\n",
    "\n",
    "We start generating a training set for binary classification problems. Consider the following function, that generates random 2D points on the plane and assign them a binary label according to their position wrt a linear separator.\n",
    "\n",
    "The use of the function is the following:\n",
    "##### X, Y = linearBinaryClass(n, low_D, high_D, m, q)\n",
    "where\n",
    "- <b>n</b> is the number of samples to be generated\n",
    "- <b>low_D</b> and <b>high_D</b> are, respectively, the lower and upper bounds for the domain of the samples\n",
    "- <b>m, q</b> are the linear function parameters\n",
    "- <b>X</b>, <b>Y</b>: 2-dimensional samples (X) associated with 1-dimensional binary labels (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBinaryClass(n, low_D, high_D, m, q):\n",
    "\n",
    "    X = np.zeros((n, 2))\n",
    "    Y = np.zeros(n)\n",
    "    for i in range(2):\n",
    "        X[:,i] = np.random.uniform(low_D, high_D, size=n)\n",
    "        \n",
    "    Y[X[:,1] - (X[:,0] * m + q) > 0] = 1 \n",
    "    Y[Y==0] = -1\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the distance between input points\n",
    "\n",
    "In order to build the KNN estimator we need to resort to a distance function.\n",
    "\n",
    "Consider a function that computes the euclidean distance between two points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidDistance(P1,P2):\n",
    "    return np.linalg.norm(P1-P2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then a function that computes all the distance between two set of points stored in two matrices. The usage is the following:\n",
    "\n",
    "##### X, Y = allDistances(X1,X2)\n",
    "where\n",
    "- <b>X1</b> is a matrix of size [n1xD], where each row is a D-dimensional point\n",
    "- <b>X2</b> is a matrix of size [n2xD], where each row is a D-dimensional point\n",
    "- <b>D</b> is a matrix of size [n1xn2], where each element (D)_ij is the distance between points (X_i, X_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDistances(X1, X2):\n",
    "    D = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for idx1 in range(len(X1)):\n",
    "        for idx2 in range(len(X2)):\n",
    "            D[idx1,idx2] = euclidDistance(X1[idx1,:],X2[idx2,:])\n",
    "    return D\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding noise to the samples\n",
    "\n",
    "As in Lab 1, we may want to perturb the data with come noise. To this purpose, we use the function <b><i>flipLabels</i></b>, that given an array of binary labels, flips the value of P% of labels (where P is a parameter in input to the function) and returns the obtained flipped array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, P):\n",
    "    if P < 1 or P > 100:\n",
    "        print(\"p should be a percentage value between 0 and 100.\")\n",
    "        return -1\n",
    "\n",
    "    if any(np.abs(Y) != 1):\n",
    "        print(\"The values of Ytr should be +1 or -1.\")\n",
    "        return -1\n",
    "\n",
    "    Y_noisy = np.copy(np.squeeze(Y))\n",
    "    if Y_noisy.ndim > 1:\n",
    "        print(\"Please supply a label array with only one dimension\")\n",
    "        return -1\n",
    "\n",
    "    n = Y_noisy.size\n",
    "    n_flips = int(np.floor(n * P / 100))\n",
    "    idx_to_flip = np.random.choice(n, size=n_flips, replace=False)\n",
    "    Y_noisy[idx_to_flip] = -Y_noisy[idx_to_flip]\n",
    "\n",
    "    return Y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The KNN classifier\n",
    "\n",
    "We are now ready to use the KNN algorithm to estimate the classification function. The use is as follow:\n",
    "\n",
    "##### Ypred = kNNClassify(Xtr, Ytr, k, Xte)\n",
    "where\n",
    "- <b>Xtr</b> is a matrix of size [ntrxD], where each row is a D-dimensional point (INPUT IN THE <b>TRAINING SET</b>)\n",
    "- <b>Ytr</b> is an array of size [ntr], where each element is a binary label (OUTPUT IN THE <b>TRAINING SET</b>)\n",
    "- <b>k</b> is the number of neighbours to be considered\n",
    "- <b>Xte</b> is a matrix of size [ntexD], where each row is a D-dimensional point (INPUT IN THE <b>TEST SET</b>)\n",
    "- <b>Ypred</b> is an array of size [nte], where each element is a binary label (ESTIMATED OUTPUT FOR THE <b>TEST SET</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(Xtr, Ytr, k, Xte):\n",
    "\n",
    "    n_train = Xtr.shape[0]\n",
    "    n_test = Xte.shape[0]\n",
    "\n",
    "    if any(np.abs(Ytr) != 1):\n",
    "        print(\"The values of Ytr should be +1 or -1.\")\n",
    "        return -1\n",
    "\n",
    "    if k > n_train:\n",
    "        print(\"k is greater than the number of points, setting k=n_train\")\n",
    "        k = n_train\n",
    "\n",
    "    Ypred = np.zeros(n_test)\n",
    "\n",
    "    dist = allDistances(Xte, Xtr)\n",
    "\n",
    "    for idx in range(n_test):\n",
    "        neigh_indexes = np.argsort(dist[idx, :])[:k]\n",
    "        avg_neigh = np.mean(Ytr[neigh_indexes])\n",
    "        Ypred[idx] = np.sign(avg_neigh)\n",
    "\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the separating function\n",
    "\n",
    "The visualization of the separating function on the training set, i.e. the function estimated by classification algorithm for discriminating between classes, is of benefit for appreciating the behavior of the binary classifier. To visualize the separating function use the following:\n",
    "\n",
    "##### separatingFkNN(Xtr, Ytr, k)\n",
    "where\n",
    "- <b>Xtr</b> is a matrix of size [ntrxD], where each row is a D-dimensional point (INPUT IN THE <b>TRAINING SET</b>)\n",
    "- <b>Ytr</b> is an array of size [ntr], where each element is a binary label (OUTPUT IN THE <b>TRAINING SET</b>)\n",
    "- <b>k</b> is the number of neighbours to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separatingFkNN(Xtr, Ytr, k):\n",
    "\n",
    "    Ypred = kNNClassify(Xtr=Xtr, Ytr=Ytr, k=k, Xte=Xtr)\n",
    "\n",
    "    x = Xtr[:, 0]\n",
    "    y = Xtr[:, 1]\n",
    "    xi = np.linspace(x.min(), x.max(), 200)\n",
    "    yi = np.linspace(y.min(), y.max(), 200)\n",
    "    zi = griddata(x, y, Ypred, xi, yi, interp='linear')\n",
    "    \n",
    "    CS = plt.contour(xi, yi, zi, 15, linewidths=2, colors='k', levels=[0])\n",
    "    # plot data points.\n",
    "    plt.scatter(x, y, c=Ytr, marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    plt.xlim(x.min(), x.max())\n",
    "    plt.ylim(x.min(), x.max())\n",
    "    msg = 'Separating function, k='+str(k);\n",
    "    plt.title(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the goodness of a classifier\n",
    "\n",
    "To evaluate how good is the classification function estimated by the KNN, we compare the predicted binary labels and expected (true) ones, with the following function:\n",
    "\n",
    "##### acc = calcAccuracy(Ypred, Ytrue)\n",
    "where\n",
    "- <b>Ypred</b> is an array of size [ntr], where each element is a binary label predicted by the classifier\n",
    "- <b>Ytrue</b> is an array of size [ntr], where each element is the true binary label\n",
    "- <b>acc</b> is the percentage of correclty classified elements wrt the total number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAccuracy(Ypred, Ytrue):\n",
    "    return (np.count_nonzero(Ypred==Ytrue))/len(Ytrue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and test sets, build and evaluate the KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "D = 2\n",
    "\n",
    "m = 0.9\n",
    "q = 0\n",
    "\n",
    "low_D = -10\n",
    "high_D = 10\n",
    "\n",
    "# Generate a training set WITHOUT NOISE\n",
    "Xtr, Ytr = linearBinaryClass(n, low_D, high_D, m, q)\n",
    "\n",
    "# Visualize the separating curve for the NN classifier \n",
    "separatingFkNN(Xtr, Ytr, 1)\n",
    "\n",
    "# Generate a test set WITHOUT NOISE\n",
    "Xte, Yte = linearBinaryClass(n, low_D, high_D, m, q)\n",
    "\n",
    "# Evaluate the NN classifier on the TEST SET\n",
    "Ypred = kNNClassify(Xtr, Ytr, 1, Xte)\n",
    "\n",
    "# Compute the accuracy on the TEST SET\n",
    "acc = calcAccuracy(Ypred, Yte)\n",
    "\n",
    "print(\"With K=1 the accuracy on the test set is \", acc)\n",
    "\n",
    "# How the classifier perform on the TRAINING SET instead?\n",
    "Ypredtr = kNNClassify(Xtr, Ytr, 1, Xtr)\n",
    "acctr = calcAccuracy(Ypredtr, Ytr)\n",
    "print(\"With K=1 the accuracy on the training set is \", acctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "Keeping the parameters of the function and the number of points as in the previous example:\n",
    "\n",
    "- 1.Generate a training set WITH NOISE (for instance with 10% of flipped labels)\n",
    "- 2.Visualize the separating curve for the <b>NN</b> classifier \n",
    "- 3.Generate a test set WITHOUT NOISE\n",
    "- 4.Evaluate the NN classifier first on the TRAINING and then on the TEST SET\n",
    "- 5.Compute the obtained accuracy first on the TRAINING and then on the TEST SET\n",
    "\n",
    "- Repeat the steps from 1 to 5 with the <b>KNN</b> algorithm, setting for instance K=5\n",
    "\n",
    "OBSERVE WHAT CHANGES...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "Analyse the performance of the <n>KNN</b> with plots, considering in particular the following:\n",
    "\n",
    "- <b>SCENARIO 1</b>: Fix the number n of points to 200, fix the amount of noise to 10%, and plot the performance of the KNN classifier on TRAINING an TEST SETS as you increase the value of K\n",
    "\n",
    "- <b>SCENARIO 2</b>: Fix the number n of points to 200, fix the value of K to a reasonable number of neighbours, and plot the performance of the KNN classifier on TRAINING an TEST SETS as you increase the amount of noise\n",
    "\n",
    "- <b>SCENARIO 3</b>: Fix noise and K to two reasonable values, fix the number of TEST samples to 300, and plot the performance of the KNN classifier on TRAINING an TEST SETS as you increase the number of TRAINING samples (e.g. from 30 to 300 with steps 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 1 goes here...    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 2 goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... your code for SCENARIO 3 goes here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
