{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. 7 - PCA and variable selection\n",
    "\n",
    "In this lab we will move to considering the problems of dimensionality reduction in unsupervised settings, and variable selection in classification (thus supervised) scenarios.\n",
    "\n",
    "As usual, we start importing libraries and functions already used in one of the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n):\n",
    "\n",
    "    means = np.array(means)\n",
    "    sigmas = np.array(sigmas)\n",
    "\n",
    "    d = means.shape[1]\n",
    "    num_classes = sigmas.size\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = np.random.multivariate_normal(mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2,\n",
    "                                                                    size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx \n",
    "        \n",
    "    if(num_classes == 2):\n",
    "        labels[labels==0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data generation\n",
    "\n",
    "To generate a synthetic dataset suitable for use with PCA and variable selection, we first generate a subset of \"relevant\" features, and then concatenate with a second set of \"dummy\" featuers. To this purpose,  we proceed as follows:\n",
    "- Generate a dataset with mixGauss using two Gaussians \"close\" with each other (see example below). Let's start considering 2-Dimensional points \n",
    "- Plot the points: you should observe an \"elongated\" points cloud\n",
    "- Enrich the input samples with \"dummy\" variables randomly sampled and concatenate to the relevant features (notice that at this point data visualization is no more possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100 # number of features for each gaussian\n",
    "d=30 # total number of features\n",
    "d_rev = 2 # number of relevant features\n",
    "\n",
    "Xtr, Ytr = mixGauss(means = [[0,0],[0,1]], sigmas = [0.7, 0.7], n=100)\n",
    "Ytr[Ytr==2]=-1\n",
    "Xte, Yte = mixGauss(means = [[0,0],[0,1]], sigmas = [0.7, 0.7], n=100)\n",
    "Yte[Yte==2]=-1\n",
    "\n",
    "# plot\n",
    "plt.scatter(Xtr[:,0], Xtr[:,1], s=30, alpha=0.80)\n",
    "plt.scatter(Xte[:,0], Xte[:,1], s=30, alpha=0.10)\n",
    "\n",
    "# dummy features generation\n",
    "sigma_noise = 0.01\n",
    "Xtr_noise = sigma_noise * np.random.randn(d_rev*n, d-d_rev)\n",
    "Xtr = np.concatenate((Xtr,Xtr_noise), axis=1)\n",
    "Xte_noise = sigma_noise * np.random.randn(d_rev*n, d-d_rev)\n",
    "Xte = np.concatenate((Xte,Xte_noise), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explainedVariance\n",
    "Given a set of eigenvalues, sorted in decreasing order, the function returns the cumulative explained variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainedVariance(eig_vals):\n",
    "    \n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i / tot)*100 for i in eig_vals]\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    \n",
    "    return cum_var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA\n",
    "\n",
    "Given a data matrix and a positive number k, the function computes the first k eigenvectors of the covariance matrix. It returns the Principal Components and the Cumulative Explained Variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, k):\n",
    "   \n",
    "    # standardize the data\n",
    "    mean = X.mean(axis=0)\n",
    "    X_z = X - mean\n",
    "    \n",
    "    # Compute the covariance matrix of X_z\n",
    "    cov_mat = # ... fill here ...\n",
    "    \n",
    "    # compute eigenvalues and eigenvectors of the covariance matrix\n",
    "    eigvals, eigvec = # ... fill here ...\n",
    "    \n",
    "    # sort the eigenvalues in decreasing order and obtain the corresponding indexes\n",
    "    \n",
    "    # ...fill here ...\n",
    "\n",
    "    \n",
    "    # select the first k eigenvectors (Principal Components)\n",
    "    \n",
    "    # ... fill here ...\n",
    "   \n",
    "    # compute the Cumulative Explained Variance\n",
    "    \n",
    "    # ... fill here ...\n",
    "    \n",
    "    return PC, expl_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA_Project\n",
    "\n",
    "Given a matrix and the Principal Components, it returns the projected points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Project(X, PC):\n",
    "\n",
    "    # standardize the data\n",
    "    mean = X.mean(axis=0)\n",
    "    X_z = X - mean\n",
    "    \n",
    "    # obtain the projected points\n",
    "    \n",
    "    # ... fill here ...\n",
    "    \n",
    "    \n",
    "    return X_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some analysis\n",
    "\n",
    "We suggest to do the following:\n",
    "- Run PCA on the training set\n",
    "- Project the points\n",
    "- Plot the Cumulative Explained Variance: what can you notice?\n",
    "- Plot the projected points: how do they look?\n",
    "\n",
    "Next, try the previous step as you\n",
    "- Increase the standard deviation (sigma_noise) when you generate dummy features\n",
    "- Increase the number of relevant features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PC, expl_var = # ... fill here ...\n",
    "X_proj = # ... fill here ...\n",
    "pcidx = np.arange(len(expl_var))\n",
    "\n",
    "plt.plot(pcidx, expl_var, '-o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Cumulative explained variance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_proj[:,0], X_proj[:,1], s=30, alpha=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OMatchingPursuit\n",
    "\n",
    "It computes a sparse representation of the signal using Orthogonal Matching Pursuit algorithm. Use it as follows:\n",
    "    \n",
    "#### w, r, I = OMatchingPursuit( X, Y, T)\n",
    "where\n",
    "    - X: input data\n",
    "    - Y: output labels\n",
    "    - T: number of iterations\n",
    "    - w: estimated coefficients\n",
    "    - r: residuals\n",
    "    - I: indices\n",
    "    \n",
    "HAVE A LOOK AT THE CODE AND TRY TO USE IT (MORE DETAILS ON MONDAY...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OMatchingPursuit(X, Y, T):\n",
    "   \n",
    "    N, D = np.shape(X)\n",
    "\n",
    "    # Initialization of residual, coefficient vector and index set I\n",
    "    r = Y\n",
    "    w = np.zeros(D)\n",
    "    I = []\n",
    "\n",
    "    for i in range(T):\n",
    "        I_tmp = range(D)\n",
    "\n",
    "        # Select the column of X which most \"explains\" the residual\n",
    "        a_max = -1\n",
    "\n",
    "        for j in I_tmp:\n",
    "            a_tmp = ((r.T.dot(X[:, j])) ** 2) / (X[:, j].T.dot(X[:, j]))\n",
    "\n",
    "            if a_tmp > a_max:\n",
    "                a_max = a_tmp\n",
    "                j_max = j\n",
    "\n",
    "        # Add the index to the set of indexes\n",
    "        if np.sum(I == j_max) == 0:\n",
    "            I.append(j_max)\n",
    "\n",
    "        # Compute the M matrix\n",
    "        M_I = np.zeros((D, D))\n",
    "\n",
    "        for j in I:\n",
    "            M_I[j, j] = 1\n",
    "\n",
    "        A = M_I.dot(X.T).dot(X).dot(M_I)\n",
    "        B = M_I.dot(X.T).dot(Y)\n",
    "\n",
    "        # Update estimated coefficients\n",
    "        w = np.linalg.pinv(A).dot(B)\n",
    "\n",
    "        # Update the residual\n",
    "        r = Y - X.dot(w)\n",
    "\n",
    "    return w, r, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual function to compute the error in predicted labels (for a binary classification problem)\n",
    "def calcError(Ypred, Y):\n",
    "\n",
    "    V=np.multiply(np.sign(Ypred),np.sign(Y) )\n",
    "    return np.count_nonzero(V<0)/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation for selecting the best value for the hyperparameters in the use of Orthogonal \n",
    "# Matching Pursuit (we refer to the number of iterations)\n",
    "\n",
    "def KFoldCVOMP(Xtr, Ytr, KF, niter_list):\n",
    "\n",
    "    if KF <= 0:\n",
    "        print(\"Please supply a positive number of repetitions\")\n",
    "        return -1\n",
    "\n",
    "    # Ensures that k_list is a numpy array\n",
    "    niter_list = np.array(niter_list)\n",
    "    num_niter = niter_list.size\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "    n_val = int(np.floor(n_tot/KF))\n",
    "\n",
    "    Tm = np.zeros(num_niter)\n",
    "    Ts = np.zeros(num_niter)\n",
    "    Vm = np.zeros(num_niter)\n",
    "    Vs = np.zeros(num_niter)\n",
    "\n",
    "    # Random permutation of training data\n",
    "    rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "    \n",
    "    \n",
    "    for kdx, niter in enumerate(niter_list):\n",
    "        first = 0\n",
    "        for fold in range(KF):\n",
    "           \n",
    "            flags = np.zeros(Xtr.shape[0])\n",
    "            flags[first:first+n_val]=1;\n",
    "            \n",
    "            X = Xtr[rand_idx[flags==0]]\n",
    "            Y = Ytr[rand_idx[flags==0]]\n",
    "            X_val = Xtr[rand_idx[flags==1]]\n",
    "            Y_val = Ytr[rand_idx[flags==1]]\n",
    "\n",
    "            # Compute the training error of the RLS regression for the given value of regpar\n",
    "            w, r, I = OMatchingPursuit(X, Y, niter)\n",
    "            YpredTR = np.sign(X.dot(w))\n",
    "            trError = calcError(YpredTR, Y)\n",
    "            Tm[kdx] = Tm[kdx] + trError\n",
    "            Ts[kdx] = Ts[kdx] + trError ** 2\n",
    "\n",
    "            # Compute the validation error of the RLS regression for the given value of regpar\n",
    "            YpredVAL = np.sign(X_val.dot(w))\n",
    "            valError = calcError(YpredVAL, Y_val)\n",
    "            Vm[kdx] = Vm[kdx] + valError\n",
    "            Vs[kdx] = Vs[kdx] + valError ** 2\n",
    "            \n",
    "            first = first+n_val                \n",
    "\n",
    "    Tm = Tm / KF\n",
    "    Ts = Ts / KF - Tm ** 2\n",
    "\n",
    "    Vm = Vm / KF\n",
    "    Vs = Vs / KF - Vm ** 2\n",
    "\n",
    "    best_niter_idx = np.argmin(Vm)\n",
    "    bestniter = niter_list[best_niter_idx]\n",
    "    \n",
    "    print(Vm)\n",
    "    print(niter_list)\n",
    "    print(best_niter_idx)\n",
    "\n",
    "    return bestniter, Vm, Vs, Tm, Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some analysis\n",
    "\n",
    "We suggest to proceed as follows:\n",
    "\n",
    "- Standardize the data\n",
    "- Run Orthogonal Matching Pursuit on the training set setting a reasonable number of iterations\n",
    "- Compute the prediction on the test set and evaluate the error\n",
    "- Plot the components of the solution w (considering their absolute value): how do they look?\n",
    "- Run the K-Fold Cross Validation to select an appropriate value for the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization\n",
    "m = np.mean(Xtr, axis=0)\n",
    "s = np.std(Xtr, axis=1)\n",
    "s = np.reshape(s, (len(s),1))\n",
    "Xtrnorm = np.divide((Xtr-m), s)\n",
    "Xtenorm = np.divide((Xte-m), s)\n",
    "\n",
    "# Run Orthogonal Matching Pursuit\n",
    "\n",
    "# ... fill here ...\n",
    "\n",
    "# Compute the prediction on the test set\n",
    "\n",
    "# ... fill here ...\n",
    "\n",
    "# Compute the test error and show its value\n",
    "\n",
    "# ... fill here ...\n",
    "\n",
    "# Plot the components of the solution w\n",
    "\n",
    "# ... fill here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an appropriate range of possible iterations number and run K-Fold Cross Validation\n",
    "KF = 5\n",
    "niter_list = # ... fill here ...\n",
    "bestniter, Vm, Vs, Tm, Ts = KFoldCVOMP(Xtr, Ytr, KF, niter_list)\n",
    "\n",
    "# Plot training and validation error\n",
    "plt.plot(niter_list, Tm)\n",
    "plt.plot(niter_list, Vm)\n",
    "print('Best number of iterations: '+str(bestniter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
