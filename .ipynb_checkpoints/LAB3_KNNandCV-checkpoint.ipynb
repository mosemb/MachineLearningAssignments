{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Cross Validation for KNN\n",
    "\n",
    "So far you experienced how the use of different possible values of K may influence the results, also depending on the type of data, their cardinality, and the amount of noise.\n",
    "\n",
    "#### We now apply Cross Validation for the choice of an appropriate value for the parameter K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "We first put in the code functions that you should recognize at this points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#from matplotlib.mlab import griddata\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidDistance(P1,P2):\n",
    "    return np.linalg.norm(P1-P2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allDistances(X1, X2):\n",
    "    D = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for idx1 in range(len(X1)):\n",
    "        for idx2 in range(len(X2)):\n",
    "            D[idx1,idx2] = euclidDistance(X1[idx1,:],X2[idx2,:])\n",
    "    return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(Xtr, Ytr, k, Xte, Te=1):\n",
    "\n",
    "    n_train = Xtr.shape[0]\n",
    "    n_test = Xte.shape[0]\n",
    "\n",
    "    if any(np.abs(Ytr) != 1):\n",
    "        print(\"The values of Ytr should be +1 or -1.\")\n",
    "        return -1\n",
    "\n",
    "    if k > n_train:\n",
    "        print(\"k is greater than the number of points, setting k=n_train\")\n",
    "        k = n_train\n",
    "\n",
    "    Ypred = np.zeros(n_test)\n",
    "\n",
    "    dist = allDistances(Xte, Xtr)\n",
    "\n",
    "    for idx in range(n_test):\n",
    "        if(Te==1): # test set\n",
    "            neigh_indexes = np.argsort(dist[idx, :])[:k]\n",
    "        else:\n",
    "            neigh_indexes = np.argsort(dist[idx, :])[1:k+1]\n",
    "            \n",
    "        avg_neigh = np.mean(Ytr[neigh_indexes])\n",
    "        Ypred[idx] = np.sign(avg_neigh)\n",
    "\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearBinaryClass(n, low_D, high_D, m, q):\n",
    "\n",
    "    X = np.zeros((n, 2))\n",
    "    Y = np.zeros(n)\n",
    "    for i in range(2):\n",
    "        X[:,i] = np.random.uniform(low_D, high_D, size=n)\n",
    "        \n",
    "    Y[X[:,1] - (X[:,0] * m + q) > 0] = 1 \n",
    "    Y[Y==0] = -1\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, P):\n",
    "    if P < 1 or P > 100:\n",
    "        print(\"p should be a percentage value between 0 and 100.\")\n",
    "        return -1\n",
    "\n",
    "    if any(np.abs(Y) != 1):\n",
    "        print(\"The values of Ytr should be +1 or -1.\")\n",
    "        return -1\n",
    "\n",
    "    Y_noisy = np.copy(np.squeeze(Y))\n",
    "    if Y_noisy.ndim > 1:\n",
    "        print(\"Please supply a label array with only one dimension\")\n",
    "        return -1\n",
    "\n",
    "    n = Y_noisy.size\n",
    "    n_flips = int(np.floor(n * P / 100))\n",
    "    idx_to_flip = np.random.choice(n, size=n_flips, replace=False)\n",
    "    Y_noisy[idx_to_flip] = -Y_noisy[idx_to_flip]\n",
    "\n",
    "    return Y_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return (np.count_nonzero(Ypred!=Ytrue))/len(Ytrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the test error for different values of K\n",
    "\n",
    "Having access to both the training and the test, we may start by having a look at the trend of the test error as the value of K increases. To this purpose we use the function\n",
    "\n",
    "##### train_err, test_err = trainTestAnalysis(Ks, Xtr, Ytr, Xte, Yte)\n",
    "where\n",
    "- <b>Ks</b>: array of possible values for K to be considered\n",
    "- <b>Xtr</b> and <b>Ytr</b>: respectively, input and output of the training set\n",
    "- <b>Xts</b> and <b>Yts</b>: respectively, input and output of the test set\n",
    "- <b>train_err</b>, <b>test_err</b>: array of errors on training and test for different values of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestAnalysis(Ks, Xtr, Ytr, Xte, Yte):\n",
    "\n",
    "    train_err = np.zeros(np.shape(Ks))\n",
    "    test_err = np.zeros(np.shape(Ks))\n",
    "\n",
    "    kpos = 0\n",
    "    for kpos in range(len(Ks)):\n",
    "        Ypredte = kNNClassify(Xtr, Ytr, Ks[kpos], Xte)\n",
    "        test_err[kpos] = calcError(Ypredte, Yte)\n",
    "        \n",
    "        Ypredtr = kNNClassify(Xtr, Ytr, Ks[kpos], Xtr)\n",
    "        train_err[kpos] = calcError(Ypredtr, Ytr)\n",
    "\n",
    "    return train_err, test_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hold-out Cross Validation\n",
    "\n",
    "For KNN, we use the following function\n",
    "\n",
    "##### best_k, Vm, Vs, Tm, Ts = holdoutCVkNN(Xtr, Ytr, perc, n_rep, k_list)\n",
    "where\n",
    "- <b>Xtr</b> and <b>Ytr</b> are respectively, input and output of the training set\n",
    "- <b>perc</b>: percentage of training set data ot be used for validation\n",
    "- <b>n_rep</b>: number of repetitions (samplings) of validation set for each value of the parameter\n",
    "- <b>k_list</b>:  array of possible values for K to be considered\n",
    "- <b>best_k</b>: the value in k_list that minimizes the mean of the validation error\n",
    "- <b>Vm</b> and <b>Vs</b>: mean and variance of the validation error for each value of the parameter\n",
    "- <b>Tm</b> and <b>Ts</b>: mean and variance of the training error for each value of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdoutCVkNN(Xtr, Ytr, perc, n_rep, k_list):\n",
    "\n",
    "    if perc < 1 or perc > 100:\n",
    "        print(\"perc should be a percentage value between 0 and 100.\")\n",
    "        return -1\n",
    "\n",
    "    if n_rep <= 0:\n",
    "        print(\"Please supply a positive number of repetitions\")\n",
    "        return -1\n",
    "\n",
    "    # Ensures that k_list is a numpy array\n",
    "    k_list = np.array(k_list)\n",
    "    num_k = k_list.size\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "    n_train = int(np.ceil(n_tot * (1 - float(perc) / 100)))\n",
    "\n",
    "    Tm = np.zeros(num_k)\n",
    "    Ts = np.zeros(num_k)\n",
    "    Vm = np.zeros(num_k)\n",
    "    Vs = np.zeros(num_k)\n",
    "\n",
    "    for kdx, k in enumerate(k_list):\n",
    "        for rip in range(n_rep):\n",
    "            # Randombly select a subset of the training set\n",
    "            rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "\n",
    "            X = Xtr[rand_idx[:n_train]]\n",
    "            Y = Ytr[rand_idx[:n_train]]\n",
    "            X_val = Xtr[rand_idx[n_train:]]\n",
    "            Y_val = Ytr[rand_idx[n_train:]]\n",
    "\n",
    "            # Compute the training error of the kNN classifier for the given value of k\n",
    "            trError = calcError(kNNClassify(X, Y, k, X), Y)\n",
    "            Tm[kdx] = Tm[kdx] + trError\n",
    "            Ts[kdx] = Ts[kdx] + trError ** 2\n",
    "\n",
    "            # Compute the validation error of the kNN classifier for the given value of k\n",
    "            valError = calcError(kNNClassify(X, Y, k, X_val), Y_val)\n",
    "            Vm[kdx] = Vm[kdx] + valError\n",
    "            Vs[kdx] = Vs[kdx] + valError ** 2\n",
    "\n",
    "    Tm = Tm / n_rep\n",
    "    Ts = Ts / n_rep - Tm ** 2\n",
    "\n",
    "    Vm = Vm / n_rep\n",
    "    Vs = Vs / n_rep - Vm ** 2\n",
    "\n",
    "    best_k_idx = np.argmin(Vm)\n",
    "    best_k = k_list[best_k_idx]\n",
    "\n",
    "    return best_k, Vm, Vs, Tm, Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "For KNN, we use the following function\n",
    "\n",
    "\n",
    "##### best_k, Vm, Vs, Tm, Ts = KFoldCVkNN(Xtr, Ytr, KF, k_list)\n",
    "where\n",
    "- <b>Xtr</b> and <b>Ytr</b> are respectively, input and output of the training set\n",
    "- <b>KF</b>: number of folds\n",
    "- <b>k_list</b>:  array of possible values for K to be considered\n",
    "- <b>best_k</b>: the value in k_list that minimizes the mean of the validation error\n",
    "- <b>Vm</b> and <b>Vs</b>: mean and variance of the validation error for each value of the parameter\n",
    "- <b>Tm</b> and <b>Ts</b>: mean and variance of the training error for each value of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCVkNN(Xtr, Ytr, KF, k_list):\n",
    "\n",
    "    if KF <= 0:\n",
    "        print(\"Please supply a positive number of repetitions\")\n",
    "        return -1\n",
    "\n",
    "    # Ensures that k_list is a numpy array\n",
    "    k_list = np.array(k_list)\n",
    "    num_k = k_list.size\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "    n_val = int(np.ceil(n_tot/KF))\n",
    "\n",
    "    Tm = np.zeros(num_k)\n",
    "    Ts = np.zeros(num_k)\n",
    "    Vm = np.zeros(num_k)\n",
    "    Vs = np.zeros(num_k)\n",
    "\n",
    "    # Random permutation of training data\n",
    "    rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "    \n",
    "    \n",
    "    for kdx, k in enumerate(k_list):\n",
    "        first = 0\n",
    "        for fold in range(KF):\n",
    "           \n",
    "            flags = np.zeros(Xtr.shape[0])\n",
    "            flags[first:first+n_val]=1;\n",
    "            \n",
    "            X = Xtr[flags==0]\n",
    "            Y = Ytr[flags==0]\n",
    "            X_val = Xtr[flags==1]\n",
    "            Y_val = Ytr[flags==1]\n",
    "\n",
    "            # Compute the training error of the kNN classifier for the given value of k\n",
    "            trError = calcError(kNNClassify(X, Y, k, X), Y)\n",
    "            Tm[kdx] = Tm[kdx] + trError\n",
    "            Ts[kdx] = Ts[kdx] + trError ** 2\n",
    "\n",
    "            # Compute the validation error of the kNN classifier for the given value of k\n",
    "            valError = calcError(kNNClassify(X, Y, k, X_val), Y_val)\n",
    "            Vm[kdx] = Vm[kdx] + valError\n",
    "            Vs[kdx] = Vs[kdx] + valError ** 2\n",
    "            \n",
    "            first = first+n_val                \n",
    "\n",
    "    Tm = Tm / n_rep\n",
    "    Ts = Ts / n_rep - Tm ** 2\n",
    "\n",
    "    Vm = Vm / n_rep\n",
    "    Vs = Vs / n_rep - Vm ** 2\n",
    "\n",
    "    best_k_idx = np.argmin(Vm)\n",
    "    k = k_list[best_k_idx]\n",
    "\n",
    "    return k, Vm, Vs, Tm, Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying model selection with different strategies\n",
    "\n",
    "It is now time to try and compare different strategies for selecting the best value for the parameter K. To this purpose we provide you some example datasets for binary classification problems (see Material.zip) you can play with. \n",
    "\n",
    "The datasets have the following properties:\n",
    "- (Training1, Test1): n=70, no noise, suggested K values are in the range 1 ... 13\n",
    "- (Training2, Test2): n=40, no noise, suggested K values are in the range 1 ... 23\n",
    "- (Training3, Test3): n=200, 20% flipped labels, suggested K values are in the range 1 ... 31\n",
    "- (Training4, Test4): n=200, 5% flipped labels, suggested K values are in the range 1 ... 19\n",
    "\n",
    "To load a dataset (e.g. Training1/Test1) use the following:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Training1.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-866ef9da53b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training1.dat\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test1.dat\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mXte\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYte\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Training1.dat'"
     ]
    }
   ],
   "source": [
    "with open(\"Training1.dat\",\"rb\") as f:\n",
    "    [Xtr, Ytr] = pickle.load(f)\n",
    "with open(\"Test1.dat\",\"rb\") as f:\n",
    "    [Xte, Yte] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "For each dataset, try and reason on the expected behavior: considering amount of data and noise, what do you expect to be a reasonable value for K? \n",
    "\n",
    "The, perform the following and compare the results:\n",
    "- Have a look at the trend of the test error as K increases (function trainTestAnalysis), plot the error and derive the value of K for which it reaches the minimum\n",
    "- Do the same using hold-out Cross Validation\n",
    "- Do the same using K-Fold Cross Validation\n",
    "\n",
    "Take a look ad the K's that you obtained in the different cases...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
