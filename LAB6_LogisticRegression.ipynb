{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Logistic Regression\n",
    "\n",
    "In this lab we implement and use logistic regressione for binary claffication problems.\n",
    "\n",
    "We start including some libraries and functions already seen in the previous labs (or slight variations of them). Have a look and verify you understand their purpose.\n",
    "\n",
    "<b>READ all the text parts very carefully, as you will find instructions on how to proceed.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n):\n",
    "\n",
    "    means = np.array(means)\n",
    "    sigmas = np.array(sigmas)\n",
    "\n",
    "    d = means.shape[1]\n",
    "    num_classes = sigmas.size\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = np.random.multivariate_normal(mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2,\n",
    "                                                                    size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx \n",
    "        \n",
    "    if(num_classes == 2):\n",
    "        labels[labels==0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, perc):\n",
    "\n",
    "    if perc < 1 or perc > 100:\n",
    "        print(\"p should be a percentage value between 0 and 100.\")\n",
    "        return -1\n",
    "\n",
    "    if any(np.abs(Y) != 1):\n",
    "        print(\"The values of Ytr should be +1 or -1.\")\n",
    "        return -1\n",
    "\n",
    "    Y_noisy = np.copy(np.squeeze(Y))\n",
    "    if Y_noisy.ndim > 1:\n",
    "        print(\"Please supply a label array with only one dimension\")\n",
    "        return -1\n",
    "\n",
    "    n = Y_noisy.size\n",
    "    n_flips = int(np.floor(n * perc / 100))\n",
    "    idx_to_flip = np.random.choice(n, size=n_flips, replace=False)\n",
    "    Y_noisy[idx_to_flip] = -Y_noisy[idx_to_flip]\n",
    "\n",
    "    return Y_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separatingFLR(Xtr, Ytr, Ypred, w):\n",
    "    \n",
    "    xi = np.linspace(Xtr[:, 0].min(), Xtr[:, 0].max(), 200)\n",
    "    yi = np.linspace(Xtr[:, 1].min(), Xtr[:, 1].max(), 200)\n",
    "    X, Y = np.meshgrid(xi,yi)\n",
    "    \n",
    "    zi = griddata(Xtr, Ypred, (X,Y), method='linear')\n",
    "    \n",
    "    plt.contour(xi, yi, zi, 15, linewidths=2, colors='k', levels=[0])\n",
    "    # plot data points.\n",
    "    plt.scatter(Xtr[:,0], Xtr[:,1], c=Ytr, marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    plt.xlim(Xtr[:,0].min(), Xtr[:,0].max())\n",
    "    plt.ylim(Xtr[:,1].min(), Xtr[:,1].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the function on the training set\n",
    "\n",
    "We define now the function <b>linearLRTrain(Xtr, Ytr, reg_par)</b> to estimate the classification function on the training set. The use is as follows:\n",
    "##### w, errs = linearLRTrain(Xtr, Ytr, reg_par)\n",
    "where\n",
    "- <b>Xtr</b> is the nxD matrix of training set inputs\n",
    "- <b>Ytr</b> is the n vector of training set outputs\n",
    "- <b>reg_par</b> is the value of the lammbda\n",
    "- <b>w</b> is the D vector of the estimated function parameters\n",
    "- <b>errs</b> is the vector of the errors made, at each iteration, in the function estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearLRTrain(Xtr, Ytr, reg_par):\n",
    "  \n",
    "    epsilon = 1e-6\n",
    "    iter = 10\n",
    "    \n",
    "    # size of the input in the trainind\n",
    "    n, D = np.shape(Xtr)\n",
    "    \n",
    "    # initialization of the vectir w\n",
    "    w = np.zeros(D)\n",
    "    \n",
    "    # estimation of the gamma parameter (fixed)\n",
    "    eigvals, eigvects = np.linalg.eig(np.dot(Xtr, np.transpose(Xtr)))\n",
    "    L = np.max(eigvals.astype(float)) / n + reg_par\n",
    "    gamma = 1/L\n",
    "    \n",
    "    # initialization of some supporting variables\n",
    "    j=0\n",
    "    f_old = 0\n",
    "    f = float(\"inf\")\n",
    "    \n",
    "    errs = np.zeros(iter+1)\n",
    "    \n",
    "    while j < iter and abs(f - f_old) >= epsilon:\n",
    "        \n",
    "        fold = f\n",
    "        j = j + 1\n",
    "        \n",
    "        w = # ... fill here ...\n",
    "        \n",
    "        f = # ... fill here ...\n",
    "        \n",
    "        #print(\"iter:\"+str(j)+\" err:\"+str(abs(f-fold)))\n",
    "        errs[j] = abs(f-fold)\n",
    "        \n",
    "    return w, errs[0:j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the function on the test set\n",
    "\n",
    "Here we define the function to evaluate the training function on a set of samples. Use it as follows\n",
    "##### Ypred, Ppred = linearLRTest(w, Xte)\n",
    "where\n",
    "- <b>w</b> is the D vector of the estimated function parameters\n",
    "- <b>Xte</b> is the matrix of input points in the test set\n",
    "- <b>Ypred</b> is the vector of predictions\n",
    "- <b>Ppred</b> is a confidence associated with each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearLRTest(w, Xtw):\n",
    "    \n",
    "    Ypred = # ... fill here ...\n",
    "    \n",
    "    # Try and understand what it does, deriving the formula\n",
    "    Ppred = np.divide(np.exp(Ypred), (1 + np.exp(Ypred)))\n",
    "    \n",
    "    return Ypred, Ppred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the quality of the prediction\n",
    "\n",
    "We need a way to quantify how good is the estimated function and its predictions. We thus define the following:\n",
    "\n",
    "##### class_err = calcError(Ypred, Y)\n",
    "where\n",
    "- <b>Ypred</b> is the vector of predictions obtained with linearLRTest\n",
    "- <b>Y</b> is the vector of true labels\n",
    "- <b>class_err</b> is the percentage of misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Y):\n",
    "    \n",
    "    class_err = # ... fill here ...\n",
    "    return class_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "A guideline maybe the following:\n",
    "\n",
    "- Build two binary classification datasets, for training and test\n",
    "- Visualize them (see examples in the notebook of the first lab if you do not remember the syntax)\n",
    "- Pick a reasonable value for the lambda parameter (e.g. reg_par = 0.1, 0.01,...)\n",
    "- Run the training\n",
    "- Plot the errors associated with each iteration\n",
    "- Show the separating curve corresponding to the obtained w (...what do you expect?)\n",
    "- Evaluate the estimated function on the test set\n",
    "- Compute and show training and test classification errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... fill here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we still missing?\n",
    "\n",
    "Think to (and possibly implement) what you should do to select an appropriate value for lambda!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... fill here ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
