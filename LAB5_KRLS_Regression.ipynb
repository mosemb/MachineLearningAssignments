{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05 - Kernel Regularized Least Squares\n",
    "\n",
    "In this lab activity we consider the extension of Regularized Least Squares to non-linear problems using kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1 - Define a function to generate D-dimensional synthetic data non non-linear regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonLinearRegrFunction(n, D, low_D, high_D, W, sigma_noise,omega,t,sigma):\n",
    "   \n",
    "    X = np.zeros((n,D))\n",
    "    for i in range(0, D):\n",
    "        X[:,i] = np.random.uniform(low_D[i], high_D[i], size=n)\n",
    "    \n",
    "    gauss_noise = np.random.normal(0, sigma_noise, size=(n,1))\n",
    "\n",
    "    Y = np.dot(X*np.exp(-sigma * t) * np.sin(omega * t), W) + gauss_noise\n",
    "     \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "D = 2\n",
    "w = np.array([1,1]).transpose()\n",
    "w.shape = (D,1)\n",
    "\n",
    "omega = 0.1 * 2 * np.pi\n",
    "t = 20\n",
    "sigma = 0.1\n",
    "\n",
    "low_D = np.array([-1, -1])\n",
    "high_D = np.array([1, 1])\n",
    "\n",
    "# Here we can compute the true function\n",
    "Xtrue, Ytrue = nonLinearRegrFunction(n, D, low_D, high_D, w, 0.8,omega,t,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <b>sqDist</b> computes all the distances between two sets of points stored in two matrices X1 and X2. Use it as follows:\n",
    "\n",
    "##### D = sqDist(X1, X2)\n",
    "where\n",
    "- X1: a matrix of size [n1xd], where each row is a d-dimensional point\n",
    "- X2: a matrix of size [n2xd], where each row is a d-dimensional point\n",
    "- D: a [n1xn2] matrix where each element (D)_ij is the distance between points (X_i, X_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqDist(X1, X2):\n",
    "    sqx = np.multiply(X1, X1) +1\n",
    "    sqy = np.multiply(X2, X2)+1\n",
    "    return np.outer(sqx, np.ones(sqy.shape[0])) + np.outer(np.ones(sqx.shape[0]), sqy.T) - 2 * np.dot(X1, X2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = sqDist(Xtrue[:,0], Xtrue[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = sqDist(Xtrue[:,1], Xtrue[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <b>kernelMatrix</b> builds the matrix of the kernel (also called Gram matrix) between two sets of points stored in two matrices X1 and X2. Use is as follows:\n",
    "\n",
    "##### K = kernelMatrix(X1, X2, kernel_type, param)\n",
    "where\n",
    "- X1, X2: collections of points on which to compute the Gram matrix\n",
    "- kernel: can be 'linear', 'polynomial' or 'gaussian'\n",
    "- param: is [] for the linear kernel, the exponent of the polynomial kernel, or the standard deviation for the gaussian kernel\n",
    "- K: Gram matrix\n",
    "\n",
    "### TASK 2 - Complete the computation of the kernel matrix depending on the specific kernel choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelMatrix(X1, X2, kernel_type, param):\n",
    "   \n",
    "    if kernel_type == 'linear':\n",
    "        return  np.dot(X1.T,X2)\n",
    "    elif kernel_type == 'polynomial':\n",
    "        return  np.power((np.dot(X1.T,X2)+1),param) \n",
    "    elif kernel_type == 'gaussian':\n",
    "        return np.exp((np.power(-np.linalg.norm(X1-X2),2))/2*param**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernelMatrix(X1, X2, 'gaussian', 2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <b>regularizedKernLSTrain</b> computes the parameters of the function estimated on the training set. Use it as follows:\n",
    "\n",
    "##### c = regularizedKernLSTrain(Xtr, Ytr, kernel, sigma, lam)\n",
    "where\n",
    "- Xtr: training input\n",
    "- Ytr: training output\n",
    "- kernel_type: type of kernel ('linear', 'polynomial', 'gaussian')\n",
    "- param: is [] for the linear kernel, the exponent of the polynomial kernel, or the standard deviation for the gaussian kernel\n",
    "- reg_par: regularization parameter\n",
    "- c: model coefficients\n",
    "\n",
    "### TASK 3 - Complete the computation of the c coefficients for the case when square loss is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedKernLSTrain(Xtr, Ytr, kernel_type, param, reg_par):\n",
    "   \n",
    "    n = Xtr.shape[0]\n",
    "    K = kernelMatrix(Xtr, Xtr, kernel_type, param)\n",
    "    Xtr = np.hstack((np.ones((Xtr.shape[0], 1)), Xtr))\n",
    "    Z = reg_par * np.eye(Xtr.shape[1])\n",
    "    c =  np.dot(np.linalg.inv(K+Z),Ytr) #c = (K+Î»I)**-1(y) \n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <b>regularizedKernLSTest</b> applies an estimated function (previously called also model) to a test set, to verify its goodness. Use it as follows:\n",
    "\n",
    "##### Ypred = regularizedKernLSTest(c, Xtr, kernel_type, param, Xte)\n",
    "where\n",
    "- c: model coefficients\n",
    "- Xtr: training input\n",
    "- kernel type: type of kernel ('linear', 'polynomial', 'gaussian')\n",
    "- param: is [] for the linear kernel, the exponent of the polynomial kernel, or the standard deviation for the gaussian kernel\n",
    "- Xte: test points\n",
    "- Ypred: predicted output on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedKernLSTest(c, Xtr, kernel_type, param, Xte):\n",
    "   \n",
    "    Ktest = kernelMatrix(Xte, Xtr, kernel_type, param)\n",
    "    Ypred = np.dot(Ktest, c)\n",
    "\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <b>calcErr</b> computes the error between real and predicted output (for regression problems). Use it as follows:\n",
    "\n",
    "##### err = calcErr(Ypred, Ytrue)\n",
    "\n",
    "where\n",
    "- Ypred: estimated (predicted) output\n",
    "- Ytrue: true (correct) output\n",
    "- err: error estimated as Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcErr(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify the function for <b>V-Fold Cross Validation</b>, you already used in the previous labs, to adapt it to the use with Kernel Regularized Least Squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VFoldCVKernRLS(x, y, KF, kernel_type, lam_list, kerpar_list):\n",
    "   \n",
    "    if KF <= 0:\n",
    "        print(\"Please supply a positive number of repetitions\")\n",
    "        return -1\n",
    "\n",
    "    if isinstance(kerpar_list, int):\n",
    "        kerpar_list = np.array([kerpar_list])\n",
    "    else:\n",
    "        kerpar_list = np.array(kerpar_list)\n",
    "    nkerpar = kerpar_list.size\n",
    "\n",
    "    if isinstance(lam_list, int):\n",
    "        lam_list = np.array([lam_list])\n",
    "    else:\n",
    "        lam_list = np.array(lam_list)\n",
    "    nlambda = lam_list.size\n",
    "\n",
    "    n = Xtr.shape[0]\n",
    "    n_val = int(np.ceil(n/KF))\n",
    "    ntr = n - n_val\n",
    "\n",
    "    tm = np.zeros((nlambda, nkerpar))\n",
    "    ts = np.zeros((nlambda, nkerpar))\n",
    "    vm = np.zeros((nlambda, nkerpar))\n",
    "    vs = np.zeros((nlambda, nkerpar))\n",
    "\n",
    "    ym = float(y.max() + y.min()) / float(2)\n",
    "    \n",
    "     # Random permutation of training data\n",
    "    rand_idx = np.random.choice(n, size=n, replace=False)\n",
    "\n",
    "    il = 0\n",
    "    for l in lam_list:\n",
    "        iss = 0\n",
    "        for s in kerpar_list:\n",
    "            trerr = np.zeros((KF, 1))\n",
    "            vlerr = np.zeros((KF, 1))\n",
    "            first=0\n",
    "            for fold in range(KF):\n",
    "                \n",
    "                flags = np.zeros(Xtr.shape[0])\n",
    "                flags[first:first+n_val]=1;\n",
    "            \n",
    "                X = Xtr[rand_idx[flags==0]]\n",
    "                Y = Ytr[rand_idx[flags==0]]\n",
    "                X_val = Xtr[rand_idx[flags==1]]\n",
    "                Y_val = Ytr[rand_idx[flags==1]]\n",
    "\n",
    "                c = regularizedKernLSTrain(X, Y, kernel_type, s, l)\n",
    "                \n",
    "                trerr[fold] = calcErr(regularizedKernLSTest(c, X, kernel_type, s, X), Y)\n",
    "                vlerr[fold] = calcErr(regularizedKernLSTest(c, X, kernel_type, s, X_val), Y_val)\n",
    "                \n",
    "            tm[il, iss] = np.median(trerr)\n",
    "            ts[il, iss] = np.std(trerr)\n",
    "            vm[il, iss] = np.median(vlerr)\n",
    "            vs[il, iss] = np.std(vlerr)\n",
    "            iss = iss + 1\n",
    "        il = il + 1\n",
    "    row, col = np.where(vm == np.amin(vm))\n",
    "    l = lam_list[row]\n",
    "    s = kerpar_list[col]\n",
    "\n",
    "    return [l, s, vm, vs, tm, ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, let's move to the analysis..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4 - Build a training and a test set using the function to generate synthetica data for non-linear regression problems you defined with TASK 1\n",
    "\n",
    "Generate, for instance, 2-dimensional data in the range [-1, 1] with sigma_noise = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr =   nonLinearRegrFunction(n, D, low_D, high_D, w, 0.8,omega,t,sigma)\n",
    "Xte, Yte =   nonLinearRegrFunction(n, D, low_D, high_D, w, 0.8,omega,t,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 5 - Complete, where required, the final parts, considering first a gaussian and then a polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-162-3241c532e5e3>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-162-3241c532e5e3>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    Ypred =  # fill here\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "kerpar_list = [0.5] # for the time being let's use only one possible value for the kernel parameter\n",
    "lam_list = [10, 7, 5, 2, 1, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001, 0.00001, 0.000001]\n",
    "V_Fold = 5\n",
    "\n",
    "l, s, vm, vs, tm, ts = VFoldCVKernRLS(x, y, KF, kernel_type, lam_list, kerpar_list)\n",
    "\n",
    "fig, axs = plt.subplots(1,  1)\n",
    "plt. title('Using gaussian kernel')\n",
    "plt.semilogx(lam_list, tm, 'r')\n",
    "plt.semilogx(lam_list, vm, 'b')\n",
    "plt.legend(['Training error', 'Validation error'])\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Now let's train the model with the best value for the parameters \n",
    "c = regularizedKernLSTrain(Xtr, Ytr, 'gaussian', param=s[0], reg_par=l[0])\n",
    "\n",
    "# Predict the output on the test set with the estimated model\n",
    "Ypred =  regularizedKernLSTest(c, Xtr, kernel_type, param, Xte)\n",
    "\n",
    "# Compute the error between predicted and real output\n",
    "err =  calcErr(Ypred, Ytr)\n",
    "\n",
    "print('Test error: '+str(err)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (200,1) not aligned: 2 (dim 1) != 200 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-0758d5c8ecf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizedKernLSTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gaussian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreg_par\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-9ee1324a06f9>\u001b[0m in \u001b[0;36mregularizedKernLSTrain\u001b[0;34m(Xtr, Ytr, kernel_type, param, reg_par)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mXtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_par\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYtr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#c = (K+Î»I)**-1(y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,2) and (200,1) not aligned: 2 (dim 1) != 200 (dim 0)"
     ]
    }
   ],
   "source": [
    "c = regularizedKernLSTrain(Xtr, Ytr, 'gaussian', param=0.9 , reg_par=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerpar_list = [4] # for the time being let's use only one possible value for the kernel parameter\n",
    "lam_list = [5, 2, 1, 0.7, 0.5, 0.3, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001,0.00001,0.000001]\n",
    "V_Fold = 5\n",
    "\n",
    "l, s, vm, vs, tm, ts = VFoldCVKernRLS( # fill here\n",
    "\n",
    "fig, axs = plt.subplots(1,  1)\n",
    "plt. title('Using polynomial kernel')\n",
    "plt.semilogx(lam_list, tm, 'r')\n",
    "plt.semilogx(lam_list, vm, 'b')\n",
    "plt.legend(['Training error', 'Validation error'])\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Now let's train the model with the best value for the parameters \n",
    "c = regularizedKernLSTrain(Xtr, Ytr, 'polynomial', param=s[0], reg_par=l[0])\n",
    "\n",
    "# Predict the output on the test set with the estimated model\n",
    "Ypred = # fill here\n",
    "\n",
    "# Compute the error between predicted and real output\n",
    "err = # fill here\n",
    "\n",
    "print('Test error: '+str(err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
